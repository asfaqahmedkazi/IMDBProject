{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE PROBLEM\n",
    "The dataset contains a collection of 50,000 reviews from IMDB. \n",
    "It contains an even number of positive and negative reviews. Actually, IMDb lets users rate movies on a scale from 1 to 10. \n",
    "To label these reviews the curator of the data, labeled anything with ≤ 4 stars as negative and anything with ≥ 7 stars as positive Reviews with 5 or 6 stars were left out. The dataset is divided into training and test sets.\n",
    "\n",
    "Our task is to look at these movie reviews and for each one, we are going to predict whether they were positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Import the libraries that we will use in this Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the important libraries\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import os,re,string\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: IMPORT DATA\n",
    "The data can be download it by running the following commands in a Jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-04 03:30:32--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘aclImdb_v1.tar.gz’\n",
      "\n",
      "aclImdb_v1.tar.gz   100%[===================>]  80.23M  22.8MB/s    in 5.2s    \n",
      "\n",
      "2021-03-04 03:30:37 (15.5 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n",
      "gzip: aclImdb_v1.tar already exists; do you wish to overwrite (y or n)? "
     ]
    }
   ],
   "source": [
    "#import data\n",
    "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!gunzip aclImdb_v1.tar.gz\n",
    "!tar -xvf aclImdb_v1.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the above commands finished you’ll see that you’ve got a train and a test directory and inside your train directory, you’ll see there is a negative and a positive directory. The ones that were strongly positive went in /pos and strongly negative went in /neg. In both directories, you’ll see there is a bunch of text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdbEr.txt  imdb.vocab\tREADME\ttest  train\n"
     ]
    }
   ],
   "source": [
    "#Declare Path & Names\n",
    "PATH='aclImdb/'\n",
    "names = ['neg','pos']\n",
    "#Check the files in the path\n",
    "!ls {PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeledBow.feat  pos\tunsupBow.feat  urls_pos.txt\n",
      "neg\t\t unsup\turls_neg.txt   urls_unsup.txt\n"
     ]
    }
   ],
   "source": [
    "#Check the path for Train Folder\n",
    "!ls {PATH}train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_9.txt\n",
      "10000_8.txt\n",
      "10001_10.txt\n",
      "10002_7.txt\n",
      "10003_8.txt\n",
      "10004_8.txt\n",
      "10005_7.txt\n",
      "10006_7.txt\n",
      "10007_7.txt\n",
      "10008_7.txt\n",
      "ls: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "#load files in Train Folder\n",
    "!ls {PATH}train/pos | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeledBow.feat  neg  pos  urls_neg.txt  urls_pos.txt\n"
     ]
    }
   ],
   "source": [
    "#Similar for the test folder\n",
    "!ls {PATH}test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_10.txt\n",
      "10000_7.txt\n",
      "10001_9.txt\n",
      "10002_8.txt\n",
      "10003_8.txt\n",
      "10004_9.txt\n",
      "10005_8.txt\n",
      "10006_7.txt\n",
      "10007_10.txt\n",
      "10008_8.txt\n",
      "ls: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "#Load files in Test folder\n",
    "!ls {PATH}test/pos | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine all the datasets into one Array\n",
    "def load_texts_labels_from_folders(path, folders):\n",
    "    texts,labels = [],[]\n",
    "    for idx,label in enumerate(folders):\n",
    "        for fname in glob(os.path.join(path, label, '*.*')):\n",
    "            texts.append(open(fname, 'r').read())\n",
    "            labels.append(idx)\n",
    "    # stored as np.int8 to save space \n",
    "    return texts, np.array(labels).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn,trn_y = load_texts_labels_from_folders(f'{PATH}train',names)\n",
    "val,val_y = load_texts_labels_from_folders(f'{PATH}test',names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000, 25000, 25000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check theNumber of Files\n",
    "len(val),len(trn_y),len(val),len(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12500, 12500)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn_y[trn_y==1]),len(val_y[val_y==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(trn_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "The data is split evenly with 25k reviews intended for training and 25k for testing your classifier. Moreover, each set has 12.5k positive and 12.5k negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull Out a File as example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is indeed a god adaptation of Jane Austen's novel. Compared with the American Version with Guinneth Paltrow, the script was written to resemble as much as possible the book. But the acting was awful. Besides Kate Beckinsale, who I believe was a true likeness of the Emma in the book, all the other actors were trying too hard. Mark Strong was not the \"gentleman\" he was supposed to be. He was often rude and offensive, had no feeling whatsoever, and throughout the entire film you could not see his love \"growing\" for Emma at all. This had a terrible effect on Kate Beckinsale, who seemed to be trying to \"resque\" her leading role as well as her partner's. Moreover, there was no chemistry between the entire cast. Hariett Smith, played by Samantha Morton, seemed to have no real attachment to Mr. Elton, played by Dominic Rowan. Therefore, she did not seem as heartbroken as she was portrayed in the book. The settings of the film are also too poor. The costumes are even more so. I would have imagined Emma Woodhouse to dress in a more fashionable and elegant way that she does here. The ending is also too long. It is good that it resembles the book's ending, but it is a killer ending for a film. And again, I can see no feeling of happiness in the face of Mr.Knightley. To conclude, I believed this adaptation to be loyal to the book, but with poor actors. It seemed as if the film was made without any budget at all. I would prefer to see the \"lighter\" version with Paltrow and Northam, even if it is clear that it was made to be a \"blockbuster\", than to watch these actors (excepting the good Olivia Williams and the better Kate Beckinsale) ruin the entire script.\n",
      "\n",
      "Review's label: 0\n"
     ]
    }
   ],
   "source": [
    "print(trn[0])\n",
    "print()\n",
    "print(f\"Review's label: {trn_y[0]}\")\n",
    "# 0 represent a negative review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text data preprocessing:\n",
    "The next step is to preprocess the movie reviews. To tackle this we will use the CountVectorizer API of Sklearn which convert a collection of text documents to a matrix of token counts. Practically, it creates a sparse bag of words matrix with the caveat that throws away all of the interesting stuff about language which is the order in which the words are in.\n",
    "\n",
    "This is very often not a good idea, but in this particular case, it’s going to turn out to work not too badly. Normally, the order of the words matters a lot. If you’ve got a “not” before something, then that “not” refers to that thing.\n",
    "\n",
    "But in this case, we are trying to predict whether something is positive or negative. If you see the word “absurd” or “cryptic” appear a lot then maybe that’s a sign that this isn’t very good. So the idea is that we are going to turn it into something called a term document matrix where for each document (i.e. each review), we are just going to create a list of what words are in it, rather than what order they are in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization:\n",
    "Before transforming our text into a term document matrix we will need to tokenize it first. \n",
    "In NLP tokenization is the process of transforming your text into a list of words. \n",
    "\n",
    "For example:\n",
    "\"This movie is good\" ---> [\"This\", \"movie\", \"is\", \"good\"]\n",
    "This looks like a trivial process however it isn't. In the case we have This \"movie\" isn’t good., how do you deal with that punctuation? A good tokenizer would turn this:\n",
    "\n",
    "\"This 'movie' isn’t good.\" ---> [\"This\", \"'\", \"movie\", \"'\", \"is\", \"n't\", \"good\",\".\"]\n",
    "Every token is either a single piece of punctuation, word or this suffix n't which is considered like a word. That’s how we would probably want to tokenize that piece of text. You wouldn’t want just to split on spaces cause it would have resulted to weird tokens like \"good.\" and \"movie\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer converts a collection of text documents to a matrix of token counts (part of sklearn.feature_extraction.text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', \"'\", 'movie', \"'\", 'isn', '’', 't', 'good', '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenizing\n",
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()\n",
    "\n",
    "#See how toeknizing works\n",
    "s = \"This 'movie' isn’t good.\" \n",
    "tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create term documetn matrix\n",
    "veczr = CountVectorizer(tokenizer=tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit_transform(trn) finds the vocabulary in the training set. It also transforms the training set into a term-document matrix. Since we have to apply the same transformation to your validation set, the second line uses just the method transform(val). trn_term_doc and val_term_doc are sparse matrices. trn_term_doc[i] represents training document i and it contains a count of words for each document for each word in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_term_doc = veczr.fit_transform(trn)\n",
    "# Important: Use same vocab for validation set\n",
    "val_term_doc = veczr.transform(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen below when we create this term document matrix, the training set has 25,000 rows because there are 25,000 movie reviews and there are 75,132 columns which is the number of unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x75132 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3749745 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#See how many unique words are there\n",
    "trn_term_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "75,132 columns that too many columns. Since most of the documents don’t have most of these 75,132 words we don’t want to actually store it as a normal array in memory. So instead, we store it as a sparse matrix.\n",
    "\n",
    "What is a sparse matrix?\n",
    "It simply stores as something that says whereabouts the non-zeros are located. For example, for the document number 1, word number 4 appears and it has 4 of them. term number 123 appears once, and so forth.\n",
    "\n",
    "(1, 4) → 4\n",
    "(1, 123) → 1\n",
    "\n",
    "That’s basically how it’s stored and the important thing is that it’s efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*******************************************************************************************************************************************************************\n",
    "Below is the Practice coding to see if the words are arranged properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x75132 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 87 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_term_doc[5] #87 stored elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We grab the sixth review and that gives us 75,132 long sparse row with 83 non-zero stored elements . So in other words, the sixth review contains 87 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'(as',\n",
       " 'a',\n",
       " 'actors!!!the',\n",
       " 'and',\n",
       " 'approach.the',\n",
       " 'are',\n",
       " 'as',\n",
       " 'awful',\n",
       " 'b-movie',\n",
       " 'been',\n",
       " 'blends',\n",
       " 'but',\n",
       " 'camera',\n",
       " 'can',\n",
       " 'childish',\n",
       " 'clise',\n",
       " 'day,there',\n",
       " 'did)',\n",
       " 'do',\n",
       " 'enjoy',\n",
       " 'even',\n",
       " 'evening',\n",
       " 'film',\n",
       " 'for',\n",
       " 'friends',\n",
       " 'full',\n",
       " 'funny',\n",
       " 'genre',\n",
       " 'go',\n",
       " 'got',\n",
       " 'green',\n",
       " 'have',\n",
       " 'i',\n",
       " 'in',\n",
       " 'inspiration(at',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'least',\n",
       " 'light',\n",
       " 'lines',\n",
       " 'movie).everything',\n",
       " 'movie,has',\n",
       " 'must',\n",
       " 'no',\n",
       " 'of',\n",
       " 'plot',\n",
       " 'poe',\n",
       " 'pointer,even',\n",
       " 'really',\n",
       " 'red',\n",
       " 'reflections',\n",
       " 'relaxing',\n",
       " \"scenery's\",\n",
       " 'scenes',\n",
       " 'see',\n",
       " 'seriously!',\n",
       " 'shallow',\n",
       " 'shot',\n",
       " 'slightest',\n",
       " 'take',\n",
       " 'that',\n",
       " 'the',\n",
       " 'then',\n",
       " \"there's\",\n",
       " 'this',\n",
       " 'thriller',\n",
       " 'thrilling.if',\n",
       " 'to',\n",
       " 'want',\n",
       " 'watch',\n",
       " 'way',\n",
       " 'where',\n",
       " 'will',\n",
       " 'with',\n",
       " 'without',\n",
       " 'you'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check what is the review for each trn_term_doc\n",
    "w0 = set([o.lower() for o in trn[5].split(' ')]); w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\"blockbuster\",',\n",
       " '\"gentleman\"',\n",
       " '\"growing\"',\n",
       " '\"lighter\"',\n",
       " '\"resque\"',\n",
       " '(excepting',\n",
       " 'a',\n",
       " 'acting',\n",
       " 'actors',\n",
       " 'actors.',\n",
       " 'adaptation',\n",
       " 'again,',\n",
       " 'all',\n",
       " 'all.',\n",
       " 'also',\n",
       " 'american',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'as',\n",
       " 'at',\n",
       " 'attachment',\n",
       " \"austen's\",\n",
       " 'awful.',\n",
       " 'be',\n",
       " 'be.',\n",
       " 'beckinsale)',\n",
       " 'beckinsale,',\n",
       " 'believe',\n",
       " 'believed',\n",
       " 'besides',\n",
       " 'better',\n",
       " 'between',\n",
       " \"book's\",\n",
       " 'book,',\n",
       " 'book.',\n",
       " 'budget',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'cast.',\n",
       " 'chemistry',\n",
       " 'clear',\n",
       " 'compared',\n",
       " 'conclude,',\n",
       " 'costumes',\n",
       " 'could',\n",
       " 'did',\n",
       " 'does',\n",
       " 'dominic',\n",
       " 'dress',\n",
       " 'effect',\n",
       " 'elegant',\n",
       " 'elton,',\n",
       " 'emma',\n",
       " 'ending',\n",
       " 'ending,',\n",
       " 'entire',\n",
       " 'even',\n",
       " 'face',\n",
       " 'fashionable',\n",
       " 'feeling',\n",
       " 'film',\n",
       " 'film.',\n",
       " 'for',\n",
       " 'god',\n",
       " 'good',\n",
       " 'guinneth',\n",
       " 'had',\n",
       " 'happiness',\n",
       " 'hard.',\n",
       " 'hariett',\n",
       " 'have',\n",
       " 'he',\n",
       " 'heartbroken',\n",
       " 'her',\n",
       " 'here.',\n",
       " 'his',\n",
       " 'i',\n",
       " 'if',\n",
       " 'imagined',\n",
       " 'in',\n",
       " 'indeed',\n",
       " 'is',\n",
       " 'it',\n",
       " 'jane',\n",
       " 'kate',\n",
       " 'killer',\n",
       " 'leading',\n",
       " 'likeness',\n",
       " 'long.',\n",
       " 'love',\n",
       " 'loyal',\n",
       " 'made',\n",
       " 'mark',\n",
       " 'more',\n",
       " 'moreover,',\n",
       " 'morton,',\n",
       " 'mr.',\n",
       " 'mr.knightley.',\n",
       " 'much',\n",
       " 'no',\n",
       " 'northam,',\n",
       " 'not',\n",
       " 'novel.',\n",
       " 'of',\n",
       " 'offensive,',\n",
       " 'often',\n",
       " 'olivia',\n",
       " 'on',\n",
       " 'other',\n",
       " 'paltrow',\n",
       " 'paltrow,',\n",
       " \"partner's.\",\n",
       " 'played',\n",
       " 'poor',\n",
       " 'poor.',\n",
       " 'portrayed',\n",
       " 'possible',\n",
       " 'prefer',\n",
       " 'real',\n",
       " 'resemble',\n",
       " 'resembles',\n",
       " 'role',\n",
       " 'rowan.',\n",
       " 'rude',\n",
       " 'ruin',\n",
       " 'samantha',\n",
       " 'script',\n",
       " 'script.',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'settings',\n",
       " 'she',\n",
       " 'smith,',\n",
       " 'so.',\n",
       " 'strong',\n",
       " 'supposed',\n",
       " 'terrible',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'there',\n",
       " 'therefore,',\n",
       " 'these',\n",
       " 'this',\n",
       " 'throughout',\n",
       " 'to',\n",
       " 'too',\n",
       " 'true',\n",
       " 'trying',\n",
       " 'version',\n",
       " 'was',\n",
       " 'watch',\n",
       " 'way',\n",
       " 'well',\n",
       " 'were',\n",
       " 'whatsoever,',\n",
       " 'who',\n",
       " 'williams',\n",
       " 'with',\n",
       " 'without',\n",
       " 'woodhouse',\n",
       " 'would',\n",
       " 'written',\n",
       " 'you'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check what is the review for each trn_term_doc\n",
    "w1 = set([o.lower() for o in trn[0].split(' ')]); w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w0)\n",
    "# length is 77 which is pretty similar to 87, and just the \n",
    "# difference will be that I didn’t use a real tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w1)\n",
    "# length is 77 which is pretty similar to 87, and just the \n",
    "# difference will be that I didn’t use a real tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x75132 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 163 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_term_doc[0] #163 stored elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn gives us the ability to have a look at vocabulary by saying veczr.get_feature_names . Here is an example of a few of the elements of feature names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75132\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['aussie', 'aussies', 'austen', 'austeniana', 'austens']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = veczr.get_feature_names()\n",
    "print(len(vocab))\n",
    "vocab[5000:5005]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simply created a unique list of words and mapped them. We could check by calling veczr.vocabulary_ to find the ID of a particular word. So this is like the reverse map of veczr.get_feature_names which maps integer to word, veczr.vocabulary_ maps word to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5234"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "veczr.vocabulary_['awful']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_term_doc[0,5234]\n",
    "# word 'absurd' appears twice in the first document\n",
    "# word 'and' appears six time in the first document\n",
    "# word 'awful' appears once in the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59356"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "veczr.vocabulary_['shallow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_term_doc[5,59356]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arching'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[4050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21789"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "veczr.vocabulary_['ending']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_term_doc[0,21789]\n",
    "# word 'ending' appears thrice in the first document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*******************************************************************************************************************************************************************\n",
    "Practice Ends Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAIVE BYERS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr(y_i):\n",
    "    p = x[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=trn_term_doc\n",
    "y=trn_y\n",
    "\n",
    "r = np.log((pr(1)/pr(0)))\n",
    "b = np.log((y==1).mean() / (y==0).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 75132), (25000, 75132))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.shape,val_term_doc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[False, False, False, ...,  True,  True,  True]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81656"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Formula for Naive Byers\n",
    "pre_preds = val_term_doc @ r.T + b\n",
    "preds = pre_preds.T>0\n",
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83016"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Binarized Naive Byers\n",
    "x=trn_term_doc.sign()\n",
    "r = np.log(pr(1)/pr(0))\n",
    "\n",
    "pre_preds = val_term_doc.sign() @ r.T + b #sign binarize\n",
    "preds = pre_preds.T>0\n",
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
